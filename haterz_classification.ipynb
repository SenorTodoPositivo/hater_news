{
 "metadata": {
  "name": "",
  "signature": "sha256:13a37bd0278eb85d6646ea748023fbad391512e6dbc0546ae14efc3e2e5be036"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](https://d13yacurqjgara.cloudfront.net/users/3093/screenshots/797096/hn-logo-dribbble-shot_1x.png) \n",
      "# Hater News\n",
      "\n",
      "Haterz gonna hate. But now you know who the haterz are."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO import the Pipeline class\n",
      "from sklearn.pipeline import Pipeline\n",
      "# TODO import TfidfVectorize\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "# TODO import Perceptron\n",
      "from sklearn.linear_model import Perceptron\n",
      "# TODO import cross_val_score\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "import pandas as pd\n",
      "# Logistic Regression or An SVM would work better LINEAR SVC\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import numpy as np\n",
      "# TODO import the CountVectorizer transformer from sklearn.feature_extraction.text\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "# Create an instance of CountVectorizer. Set the keyword argument `binary` to True.\n",
      "\n",
      "# Getting Stopwords to improve accuracy\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "\n",
      "# TODO if you do not have NLTK, you can install it using `pip install nltk`.\n",
      "# TODO import the functions `word_tokenize` and `pos_tag` from the module `nltk`.\n",
      "from nltk import word_tokenize, pos_tag\n",
      "# TODO import the class `PorterStemmer` from NLTK's `stem` module.\n",
      "from nltk.stem import PorterStemmer\n",
      "# TODO import the class `WordNetLemmatizer` from `nltk.stem.wordnet`.\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "\n",
      "\n",
      "stopwords = stopwords.words('english')\n",
      "\n",
      "corpus = pd.read_csv('data/total_data.csv')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Setting Up my X & y\n",
      "X_raw = corpus.Comment\n",
      "y_raw = corpus.Insult\n",
      "\n",
      "\n",
      "# # Messing With Stuff To Improve My Scorezzz\n",
      "# mynewtext = []\n",
      "\n",
      "# for number in range(len(X_raw)) w: \n",
      "#     for w in X_raw[number] if w not in stopwords:\n",
      "#       mynewtext.append(w)\n",
      "        \n",
      "# stemmer = PorterStemmer()\n",
      "# lemmatizer = WordNetLemmatizer()\n",
      "   \n",
      "    \n",
      "    \n",
      "# Setting Up My Different Pipelines\n",
      "pipeline = Pipeline([\n",
      "                     ('vect', TfidfVectorizer(binary=True, use_idf=True, stop_words='english')),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "\n",
      "other_pipeline = Pipeline([\n",
      "                     ('vect', CountVectorizer(binary=True)),\n",
      "                     ('clf', LogisticRegression())\n",
      "])\n",
      "\n",
      "\n",
      "LSVC_pipeline = Pipeline([\n",
      "                     ('vect',  TfidfVectorizer(binary=True, use_idf=True, stop_words='english')),\n",
      "                     ('clf', LinearSVC())\n",
      "])\n",
      "\n",
      "Perceptron_pipeline = Pipeline([\n",
      "                     ('vect',  TfidfVectorizer(binary=True, use_idf=True, stop_words='english')),\n",
      "                     ('clf', Perceptron())\n",
      "])\n",
      "\n",
      "# EVALUATION TIME!!\n",
      "\n",
      "# My STATNDARD THUS FAR\n",
      "accuracy_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5)\n",
      "precision_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "recall_scores = cross_val_score(pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "\n",
      "print \"TfidfVect + Logistic Regression\"\n",
      "print \"-------------------------------\"\n",
      "print \"Accuracy:\", accuracy_scores, \"Mean:\", np.mean(accuracy_scores)\n",
      "print \"Precision:\", precision_scores, \"Mean:\", np.mean(precision_scores)\n",
      "print \"Recall:\", recall_scores, \"Mean:\", np.mean(recall_scores)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# VS NORMAL COUNT VECTORIZER\n",
      "\n",
      "other_accuracy_scores = cross_val_score(other_pipeline, X_raw, y_raw, cv=5)\n",
      "other_precision_scores = cross_val_score(other_pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "other_recall_scores = cross_val_score(other_pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print\n",
      "print\n",
      "print \"CountVect + Logistic Regression\"\n",
      "print \"-------------------------------\"\n",
      "print \"Accuracy:\", other_accuracy_scores, \"Mean:\", np.mean(other_accuracy_scores)\n",
      "print \"Precision:\", other_precision_scores, \"Mean:\", np.mean(other_precision_scores)\n",
      "print \"Recall:\", other_recall_scores, \"Mean:\", np.mean(other_recall_scores)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# VS NORMAL COUNT VECTORIZER\n",
      "\n",
      "LSVC_accuracy_scores = cross_val_score(LSVC_pipeline, X_raw, y_raw, cv=5)\n",
      "LSVC_precision_scores = cross_val_score(LSVC_pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "LSVC_recall_scores = cross_val_score(LSVC_pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print\n",
      "print\n",
      "print \"TfidfVect + LinearSVC CRAP\"\n",
      "print \"-------------------------------\"\n",
      "print \"Accuracy:\", LSVC_accuracy_scores, \"Mean:\", np.mean(LSVC_accuracy_scores)\n",
      "print \"Precision:\", LSVC_precision_scores, \"Mean:\", np.mean(LSVC_precision_scores)\n",
      "print \"Recall:\", LSVC_recall_scores, \"Mean:\", np.mean(LSVC_recall_scores)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# VS NORMAL COUNT VECTORIZER\n",
      "\n",
      "Perceptron_accuracy_scores = cross_val_score(Perceptron_pipeline, X_raw, y_raw, cv=5)\n",
      "Perceptron_precision_scores = cross_val_score(Perceptron_pipeline, X_raw, y_raw, cv=5, scoring='precision')\n",
      "Perceptron_recall_scores = cross_val_score(Perceptron_pipeline, X_raw, y_raw, cv=5, scoring='recall')\n",
      "print\n",
      "print\n",
      "print \"TfidfVect + Percptron CRAP\"\n",
      "print \"-------------------------------\"\n",
      "print \"Accuracy:\", Perceptron_accuracy_scores, \"Mean:\", np.mean(Perceptron_accuracy_scores)\n",
      "print \"Precision:\", Perceptron_precision_scores, \"Mean:\", np.mean(Perceptron_precision_scores)\n",
      "print \"Recall:\", Perceptron_recall_scores, \"Mean:\", np.mean(Perceptron_recall_scores)\n",
      "\n",
      "\n",
      "\n",
      "# TODO now compare the performance of the model using binary term frequencies, TF weights, and TF-IDF weights.\n",
      "# Grid search to optimize the hyperparameters of TfidfVectorizer.\n",
      "# Which features performed best?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TfidfVect + Logistic Regression\n",
        "-------------------------------\n",
        "Accuracy: [ 0.81969697  0.81439394  0.81790592  0.81942337  0.81866464] Mean: 0.818016967858\n",
        "Precision: [ 0.85806452  0.825       0.84615385  0.8313253   0.84713376] Mean: 0.84153548429\n",
        "Recall: [ 0.38108883  0.3782235   0.37931034  0.39655172  0.38218391] Mean: 0.383471659586\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " CountVect + Logistic Regression\n",
        "-------------------------------\n",
        "Accuracy: [ 0.83939394  0.84848485  0.84218513  0.84825493  0.84066768] Mean: 0.843797305375\n",
        "Precision: [ 0.75464684  0.76895307  0.74137931  0.75694444  0.73793103] Mean: 0.751970939603\n",
        "Recall: [ 0.58166189  0.61031519  0.61781609  0.62643678  0.61494253] Mean: 0.610234495933\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TfidfVect + LinearSVC CRAP\n",
        "-------------------------------\n",
        "Accuracy: [ 0.83181818  0.82878788  0.8323217   0.84066768  0.82776935] Mean: 0.832272957189\n",
        "Precision: [ 0.74329502  0.743083    0.73605948  0.75367647  0.7122807 ] Mean: 0.737678935001\n",
        "Recall: [ 0.55587393  0.53868195  0.56896552  0.58908046  0.58333333] Mean: 0.567187036854\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TfidfVect + Percptron CRAP\n",
        "-------------------------------\n",
        "Accuracy: [ 0.8         0.80606061  0.79742033  0.79817906  0.79362671] Mean: 0.799057341242\n",
        "Precision: [ 0.64214047  0.63716814  0.6231003   0.61452514  0.60795455] Mean: 0.624977719778\n",
        "Recall: [ 0.55014327  0.61891117  0.58908046  0.63218391  0.61494253] Mean: 0.601052267562\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>Insult</th>\n",
        "      <th>Date</th>\n",
        "      <th>Comment</th>\n",
        "      <th>Usage</th>\n",
        "      <th>Unnamed: 4</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0   </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1   </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120618222256Z</td>\n",
        "      <td> \"@ian21\\xa0\"Roger Clemens is the fucking man, ...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2   </th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120618213617Z</td>\n",
        "      <td> \"Agree with Alan you are an extremest idiot.  ...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3   </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4   </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120620003825Z</td>\n",
        "      <td> \"Really suck isn't the word, when many of our ...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5   </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"...after a couple guys like him rape him firs...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6   </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120529201500Z</td>\n",
        "      <td>  \"Matthews mentored Perry at USC.  Bodes well...\"</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7   </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120619145613Z</td>\n",
        "      <td> \"How about we accept facts and patiently wait ...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8   </th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120609165037Z</td>\n",
        "      <td> \"To engage in an intelligent debate with you i...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9   </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120612145537Z</td>\n",
        "      <td> \"Same reason no one cared Obama did cocaine, w...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120610100807Z</td>\n",
        "      <td> \"Well, Kris, you might just be surprised. Can ...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120610121904Z</td>\n",
        "      <td> \"In reality GWB put more people on food stamps...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12  </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"because the zionists will blow us up if we do...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120528014019Z</td>\n",
        "      <td> \"Thank you.  Unfortunately on this kindle I'm ...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120610001759Z</td>\n",
        "      <td> \"CNN supports HARCORE MASS MURDERING ISLAMIST ...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120529020515Z</td>\n",
        "      <td>                         \"What is your problem AW\"</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120529003207Z</td>\n",
        "      <td>     \"Name a corporation that has told the truth.\"</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120528042038Z</td>\n",
        "      <td> \"You know, I don't think I ever actually put m...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120529051354Z</td>\n",
        "      <td>   \"then they shouldnt get mad when they read it.\"</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120612030048Z</td>\n",
        "      <td> \"Richards passes it to Carter. \\xa0Wow. \\xa0Su...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20  </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"I\\\\'m a second generation German working on m...</td>\n",
        "      <td>  PublicTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21  </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"@Fenestrate Even tough Mitt doesn't drink. I ...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120528221803Z</td>\n",
        "      <td>           \"Then you don't understand basketball.\"</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23  </th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120320145923Z</td>\n",
        "      <td> \"GO SUCK_YOU DADDYS_COCK_OBAMA LOVING LIBERAL_...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120619181302Z</td>\n",
        "      <td>                          \"yeah!!!! fuck undying!\"</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25  </th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"&lt;b&gt;Circumstances under which abortion is lega...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120610161043Z</td>\n",
        "      <td> \"Koolnightie,\\n\\nYou, just like your bedmate P...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27  </th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120610080925Z</td>\n",
        "      <td> \"i live with my dad asshole!!! i anal fuked th...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28  </th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120530023620Z</td>\n",
        "      <td> \"Those plays are easy for Crawford. It's the r...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29  </th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120619015242Z</td>\n",
        "      <td> \"You really think shes speaking spanish? You a...</td>\n",
        "      <td> PrivateTest</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>...</th>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "      <td>...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6564</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120619192544Z</td>\n",
        "      <td>                                        \"No shit!\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6565</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120610042233Z</td>\n",
        "      <td> \"clark just needs to stop pushing and pulling ...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6566</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120610154957Z</td>\n",
        "      <td>                                          \"faggot\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6567</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120530140143Z</td>\n",
        "      <td> \"You really should be like the other Lemmings ...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6568</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120609221726Z</td>\n",
        "      <td>             \"by your logic also legalize Heroin.\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6569</th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"The G.O.P. can cry all they want but ther the...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6570</th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td>                   \"@mikieboy - In a nutshell :-)\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6571</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120530010840Z</td>\n",
        "      <td> \"@Crissa:disqus LaRaza (The Race), NBP. Nation...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6572</th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"Hey, if the Republicans are determined to sta...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6573</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120527153826Z</td>\n",
        "      <td> \"Nah. Good folks there, good music POURS from ...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6574</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120619003831Z</td>\n",
        "      <td> \"Hey nutbag..youre a traitor....\\n\\nObama has ...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6575</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120610125954Z</td>\n",
        "      <td>                   \"You're avatar is fail. Loser.\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6576</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120619022522Z</td>\n",
        "      <td> \"Why don't you shut your stupid mouth, pennyla...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6577</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120529160326Z</td>\n",
        "      <td> \"We didn't even know who the rebels really are...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6578</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120618222326Z</td>\n",
        "      <td> \"You goofy Repub thugs don't have a clue, not ...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6579</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120527154034Z</td>\n",
        "      <td> \"socaljim:&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Nope, Kelly shou...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6580</th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"@Ritual Reality No I am simply saying that ba...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6581</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120610083604Z</td>\n",
        "      <td> \"True, maybe he's deliberately being an assh*l...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6582</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120515160649Z</td>\n",
        "      <td> \"JoelWeltman, you look like a pedophile from y...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6583</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120529001033Z</td>\n",
        "      <td> \"your a little high on the points but I do thi...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6584</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120619145323Z</td>\n",
        "      <td> \"Your Yellowstone Fly Fishing Report:\\n\\n.. Th...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6585</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120612001129Z</td>\n",
        "      <td> \"MrO,\\n\\nProof is shown by liberals not wantin...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6586</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120619205630Z</td>\n",
        "      <td> \"The only ignorant person here is you, who thi...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6587</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120610114639Z</td>\n",
        "      <td>            \"oh i had many cars like this before.\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6588</th>\n",
        "      <td> 0</td>\n",
        "      <td>             NaN</td>\n",
        "      <td> \"@Sara Besleaga Griji, doruri sau dorin\\\\xc8\\\\...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6589</th>\n",
        "      <td> 1</td>\n",
        "      <td> 20120502172717Z</td>\n",
        "      <td> \"you are both morons and that is never happening\"</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6590</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120528164814Z</td>\n",
        "      <td> \"Many toolbars include spell check, like Yahoo...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6591</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120620142813Z</td>\n",
        "      <td> \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6592</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120528205648Z</td>\n",
        "      <td> \"How about Felix? He is sure turning into one ...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6593</th>\n",
        "      <td> 0</td>\n",
        "      <td> 20120515200734Z</td>\n",
        "      <td> \"You're all upset, defending this hipster band...</td>\n",
        "      <td>         NaN</td>\n",
        "      <td>NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>6594 rows \u00d7 5 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "      Insult             Date  \\\n",
        "0          0              NaN   \n",
        "1          0  20120618222256Z   \n",
        "2          1  20120618213617Z   \n",
        "3          0              NaN   \n",
        "4          0  20120620003825Z   \n",
        "5          0              NaN   \n",
        "6          0  20120529201500Z   \n",
        "7          0  20120619145613Z   \n",
        "8          1  20120609165037Z   \n",
        "9          0  20120612145537Z   \n",
        "10         0  20120610100807Z   \n",
        "11         0  20120610121904Z   \n",
        "12         0              NaN   \n",
        "13         0  20120528014019Z   \n",
        "14         0  20120610001759Z   \n",
        "15         0  20120529020515Z   \n",
        "16         0  20120529003207Z   \n",
        "17         0  20120528042038Z   \n",
        "18         0  20120529051354Z   \n",
        "19         0  20120612030048Z   \n",
        "20         0              NaN   \n",
        "21         0              NaN   \n",
        "22         0  20120528221803Z   \n",
        "23         1  20120320145923Z   \n",
        "24         0  20120619181302Z   \n",
        "25         0              NaN   \n",
        "26         0  20120610161043Z   \n",
        "27         1  20120610080925Z   \n",
        "28         0  20120530023620Z   \n",
        "29         1  20120619015242Z   \n",
        "...      ...              ...   \n",
        "6564       0  20120619192544Z   \n",
        "6565       0  20120610042233Z   \n",
        "6566       1  20120610154957Z   \n",
        "6567       1  20120530140143Z   \n",
        "6568       0  20120609221726Z   \n",
        "6569       0              NaN   \n",
        "6570       0              NaN   \n",
        "6571       1  20120530010840Z   \n",
        "6572       0              NaN   \n",
        "6573       0  20120527153826Z   \n",
        "6574       1  20120619003831Z   \n",
        "6575       1  20120610125954Z   \n",
        "6576       1  20120619022522Z   \n",
        "6577       0  20120529160326Z   \n",
        "6578       1  20120618222326Z   \n",
        "6579       0  20120527154034Z   \n",
        "6580       0              NaN   \n",
        "6581       1  20120610083604Z   \n",
        "6582       1  20120515160649Z   \n",
        "6583       0  20120529001033Z   \n",
        "6584       0  20120619145323Z   \n",
        "6585       0  20120612001129Z   \n",
        "6586       0  20120619205630Z   \n",
        "6587       0  20120610114639Z   \n",
        "6588       0              NaN   \n",
        "6589       1  20120502172717Z   \n",
        "6590       0  20120528164814Z   \n",
        "6591       0  20120620142813Z   \n",
        "6592       0  20120528205648Z   \n",
        "6593       0  20120515200734Z   \n",
        "\n",
        "                                                Comment        Usage  \\\n",
        "0     \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...   PublicTest   \n",
        "1     \"@ian21\\xa0\"Roger Clemens is the fucking man, ...   PublicTest   \n",
        "2     \"Agree with Alan you are an extremest idiot.  ...   PublicTest   \n",
        "3     \"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...  PrivateTest   \n",
        "4     \"Really suck isn't the word, when many of our ...  PrivateTest   \n",
        "5     \"...after a couple guys like him rape him firs...  PrivateTest   \n",
        "6      \"Matthews mentored Perry at USC.  Bodes well...\"  PrivateTest   \n",
        "7     \"How about we accept facts and patiently wait ...  PrivateTest   \n",
        "8     \"To engage in an intelligent debate with you i...   PublicTest   \n",
        "9     \"Same reason no one cared Obama did cocaine, w...   PublicTest   \n",
        "10    \"Well, Kris, you might just be surprised. Can ...  PrivateTest   \n",
        "11    \"In reality GWB put more people on food stamps...  PrivateTest   \n",
        "12    \"because the zionists will blow us up if we do...  PrivateTest   \n",
        "13    \"Thank you.  Unfortunately on this kindle I'm ...  PrivateTest   \n",
        "14    \"CNN supports HARCORE MASS MURDERING ISLAMIST ...   PublicTest   \n",
        "15                            \"What is your problem AW\"  PrivateTest   \n",
        "16        \"Name a corporation that has told the truth.\"  PrivateTest   \n",
        "17    \"You know, I don't think I ever actually put m...  PrivateTest   \n",
        "18      \"then they shouldnt get mad when they read it.\"  PrivateTest   \n",
        "19    \"Richards passes it to Carter. \\xa0Wow. \\xa0Su...  PrivateTest   \n",
        "20    \"I\\\\'m a second generation German working on m...   PublicTest   \n",
        "21    \"@Fenestrate Even tough Mitt doesn't drink. I ...  PrivateTest   \n",
        "22              \"Then you don't understand basketball.\"  PrivateTest   \n",
        "23    \"GO SUCK_YOU DADDYS_COCK_OBAMA LOVING LIBERAL_...  PrivateTest   \n",
        "24                             \"yeah!!!! fuck undying!\"  PrivateTest   \n",
        "25    \"<b>Circumstances under which abortion is lega...  PrivateTest   \n",
        "26    \"Koolnightie,\\n\\nYou, just like your bedmate P...  PrivateTest   \n",
        "27    \"i live with my dad asshole!!! i anal fuked th...  PrivateTest   \n",
        "28    \"Those plays are easy for Crawford. It's the r...  PrivateTest   \n",
        "29    \"You really think shes speaking spanish? You a...  PrivateTest   \n",
        "...                                                 ...          ...   \n",
        "6564                                         \"No shit!\"          NaN   \n",
        "6565  \"clark just needs to stop pushing and pulling ...          NaN   \n",
        "6566                                           \"faggot\"          NaN   \n",
        "6567  \"You really should be like the other Lemmings ...          NaN   \n",
        "6568              \"by your logic also legalize Heroin.\"          NaN   \n",
        "6569  \"The G.O.P. can cry all they want but ther the...          NaN   \n",
        "6570                    \"@mikieboy - In a nutshell :-)\"          NaN   \n",
        "6571  \"@Crissa:disqus LaRaza (The Race), NBP. Nation...          NaN   \n",
        "6572  \"Hey, if the Republicans are determined to sta...          NaN   \n",
        "6573  \"Nah. Good folks there, good music POURS from ...          NaN   \n",
        "6574  \"Hey nutbag..youre a traitor....\\n\\nObama has ...          NaN   \n",
        "6575                    \"You're avatar is fail. Loser.\"          NaN   \n",
        "6576  \"Why don't you shut your stupid mouth, pennyla...          NaN   \n",
        "6577  \"We didn't even know who the rebels really are...          NaN   \n",
        "6578  \"You goofy Repub thugs don't have a clue, not ...          NaN   \n",
        "6579  \"socaljim:<div><br></div><div>Nope, Kelly shou...          NaN   \n",
        "6580  \"@Ritual Reality No I am simply saying that ba...          NaN   \n",
        "6581  \"True, maybe he's deliberately being an assh*l...          NaN   \n",
        "6582  \"JoelWeltman, you look like a pedophile from y...          NaN   \n",
        "6583  \"your a little high on the points but I do thi...          NaN   \n",
        "6584  \"Your Yellowstone Fly Fishing Report:\\n\\n.. Th...          NaN   \n",
        "6585  \"MrO,\\n\\nProof is shown by liberals not wantin...          NaN   \n",
        "6586  \"The only ignorant person here is you, who thi...          NaN   \n",
        "6587             \"oh i had many cars like this before.\"          NaN   \n",
        "6588  \"@Sara Besleaga Griji, doruri sau dorin\\\\xc8\\\\...          NaN   \n",
        "6589  \"you are both morons and that is never happening\"          NaN   \n",
        "6590  \"Many toolbars include spell check, like Yahoo...          NaN   \n",
        "6591  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...          NaN   \n",
        "6592  \"How about Felix? He is sure turning into one ...          NaN   \n",
        "6593  \"You're all upset, defending this hipster band...          NaN   \n",
        "\n",
        "      Unnamed: 4  \n",
        "0            NaN  \n",
        "1            NaN  \n",
        "2            NaN  \n",
        "3            NaN  \n",
        "4            NaN  \n",
        "5            NaN  \n",
        "6            NaN  \n",
        "7            NaN  \n",
        "8            NaN  \n",
        "9            NaN  \n",
        "10           NaN  \n",
        "11           NaN  \n",
        "12           NaN  \n",
        "13           NaN  \n",
        "14           NaN  \n",
        "15           NaN  \n",
        "16           NaN  \n",
        "17           NaN  \n",
        "18           NaN  \n",
        "19           NaN  \n",
        "20           NaN  \n",
        "21           NaN  \n",
        "22           NaN  \n",
        "23           NaN  \n",
        "24           NaN  \n",
        "25           NaN  \n",
        "26           NaN  \n",
        "27           NaN  \n",
        "28           NaN  \n",
        "29           NaN  \n",
        "...          ...  \n",
        "6564         NaN  \n",
        "6565         NaN  \n",
        "6566         NaN  \n",
        "6567         NaN  \n",
        "6568         NaN  \n",
        "6569         NaN  \n",
        "6570         NaN  \n",
        "6571         NaN  \n",
        "6572         NaN  \n",
        "6573         NaN  \n",
        "6574         NaN  \n",
        "6575         NaN  \n",
        "6576         NaN  \n",
        "6577         NaN  \n",
        "6578         NaN  \n",
        "6579         NaN  \n",
        "6580         NaN  \n",
        "6581         NaN  \n",
        "6582         NaN  \n",
        "6583         NaN  \n",
        "6584         NaN  \n",
        "6585         NaN  \n",
        "6586         NaN  \n",
        "6587         NaN  \n",
        "6588         NaN  \n",
        "6589         NaN  \n",
        "6590         NaN  \n",
        "6591         NaN  \n",
        "6592         NaN  \n",
        "6593         NaN  \n",
        "\n",
        "[6594 rows x 5 columns]"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Ignore The Stuff Below.\n",
      "I was messing with stuff from Andreas Mueller and this blog post: http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # From Andreas Mueller and this blog post: http://blog.kaggle.com/2012/09/26/impermium-andreas-blog/\n",
      "# from sklearn.feature_selection import SelectPercentile\n",
      "\n",
      "# select = SelectPercentile(score_func=chi2, percentile=18)\n",
      "# clf = LogisticRegression(tol=1e-8, penalty='l2', C=7)\n",
      "# countvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char\", binary=False)\n",
      "# badwords = BadWordCounter()\n",
      "# ft = FeatureStacker([(\"badwords\", badwords), (\"chars\", countvect_char), ])\n",
      "# char_model = Pipeline([('vect', ft), ('select', select), ('logr', clf)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# # IGNORE ALL OF DISSSSSS\n",
      "\n",
      "# import numpy as np\n",
      "# from scipy import sparse\n",
      "# import re\n",
      "\n",
      "# import nltk\n",
      "# import nltk.collocations as col\n",
      "# import enchant\n",
      "# #from sklearn.feature_selection import SelectPercentile, chi2\n",
      "\n",
      "# from sklearn.base import BaseEstimator\n",
      "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "# from sklearn.preprocessing import MinMaxScaler\n",
      "\n",
      "# from util import load_subjectivity\n",
      "\n",
      "# from IPython.core.debugger import Tracer\n",
      "\n",
      "# tracer = Tracer()\n",
      "\n",
      "\n",
      "# def remove_non_ascii(s):\n",
      "#     return \"\".join(i for i in s if ord(i) < 128)\n",
      "\n",
      "\n",
      "# class DensifyTransformer(BaseEstimator):\n",
      "#     def fit(self, X, y=None):\n",
      "#         return self\n",
      "\n",
      "#     def transform(self, X):\n",
      "#         if sparse.issparse(X):\n",
      "#             X = X.toarray()\n",
      "#         return X\n",
      "\n",
      "\n",
      "# class BadWordCounter(BaseEstimator):\n",
      "#     def __init__(self):\n",
      "#         with open(\"my_badlist.txt\") as f:\n",
      "#             badwords = [l.strip() for l in f.readlines()]\n",
      "#         self.badwords_ = badwords\n",
      "\n",
      "#     def get_feature_names(self):\n",
      "#         return np.array(['n_words', 'n_chars', 'allcaps', 'max_len',\n",
      "#             'mean_len', '@', '!', 'spaces', 'bad_ratio', 'n_bad',\n",
      "#             'capsratio'])\n",
      "\n",
      "#     def fit(self, documents, y=None):\n",
      "#         return self\n",
      "\n",
      "#     def transform(self, documents):\n",
      "#         ## some handcrafted features!\n",
      "#         n_words = [len(c.split()) for c in documents]\n",
      "#         n_chars = [len(c) for c in documents]\n",
      "#         # number of uppercase words\n",
      "#         allcaps = [np.sum([w.isupper() for w in comment.split()])\n",
      "#                for comment in documents]\n",
      "#         # longest word\n",
      "#         max_word_len = [np.max([len(w) for w in c.split()]) for c in documents]\n",
      "#         # average word length\n",
      "#         mean_word_len = [np.mean([len(w) for w in c.split()])\n",
      "#                                             for c in documents]\n",
      "#         # number of google badwords:\n",
      "#         n_bad = [np.sum([c.lower().count(w) for w in self.badwords_])\n",
      "#                                                 for c in documents]\n",
      "#         exclamation = [c.count(\"!\") for c in documents]\n",
      "#         addressing = [c.count(\"@\") for c in documents]\n",
      "#         spaces = [c.count(\" \") for c in documents]\n",
      "\n",
      "#         allcaps_ratio = np.array(allcaps) / np.array(n_words, dtype=np.float)\n",
      "#         bad_ratio = np.array(n_bad) / np.array(n_words, dtype=np.float)\n",
      "\n",
      "#         return np.array([n_words, n_chars, allcaps, max_word_len,\n",
      "#             mean_word_len, exclamation, addressing, spaces, bad_ratio, n_bad,\n",
      "#             allcaps_ratio]).T\n",
      "\n",
      "\n",
      "# class FeatureStacker(BaseEstimator):\n",
      "#     \"\"\"Stacks several transformer objects to yield concatenated features.\n",
      "#     Similar to pipeline, a list of tuples ``(name, estimator)`` is passed\n",
      "#     to the constructor.\n",
      "#     \"\"\"\n",
      "#     def __init__(self, transformer_list):\n",
      "#         self.transformer_list = transformer_list\n",
      "\n",
      "#     def get_feature_names(self):\n",
      "#         pass\n",
      "\n",
      "#     def fit(self, X, y=None):\n",
      "#         for name, trans in self.transformer_list:\n",
      "#             trans.fit(X, y)\n",
      "#         return self\n",
      "\n",
      "#     def transform(self, X):\n",
      "#         features = []\n",
      "#         for name, trans in self.transformer_list:\n",
      "#             features.append(trans.transform(X))\n",
      "#         issparse = [sparse.issparse(f) for f in features]\n",
      "#         if np.any(issparse):\n",
      "#             features = sparse.hstack(features).tocsr()\n",
      "#         else:\n",
      "#             features = np.hstack(features)\n",
      "#         return features\n",
      "\n",
      "#     def get_params(self, deep=True):\n",
      "#         if not deep:\n",
      "#             return super(FeatureStacker, self).get_params(deep=False)\n",
      "#         else:\n",
      "#             out = dict(self.transformer_list)\n",
      "#             for name, trans in self.transformer_list:\n",
      "#                 for key, value in trans.get_params(deep=True).iteritems():\n",
      "#                     out['%s__%s' % (name, key)] = value\n",
      "#             return out\n",
      "\n",
      "\n",
      "# def make_collocation_analyzer(collocations, length=2):\n",
      "#     def analyzer(document):\n",
      "#         cols = [bigram for bigram in nltk.ngrams(document, length)\n",
      "#                     if bigram in collocations]\n",
      "#         return cols\n",
      "\n",
      "#     return analyzer\n",
      "\n",
      "\n",
      "# class TextFeatureTransformer(BaseEstimator):\n",
      "#     def __init__(self):\n",
      "#         self.d = enchant.Dict(\"en_US\")\n",
      "#         with open(\"my_badlist.txt\") as f:\n",
      "#             badwords = [l.strip() for l in f.readlines()]\n",
      "#         self.badwords_ = badwords\n",
      "#         self.subjectivity = load_subjectivity()\n",
      "#         self.stemmer = nltk.stem.PorterStemmer()\n",
      "\n",
      "#     def get_feature_names(self):\n",
      "#         feature_names = []\n",
      "#         feature_names.extend(self.unigram_vect.get_feature_names())\n",
      "#         feature_names.extend(self.bigram_vect_you.get_feature_names())\n",
      "#         feature_names.extend(self.trigram_vect_you.get_feature_names())\n",
      "#         feature_names.extend([\"you_are_\" + w for w in\n",
      "#             self.you_are_vect.get_feature_names()])\n",
      "#         #feature_names.extend(self.pos_vect.get_feature_names())\n",
      "#         feature_names.extend([\"n_nicks\", \"n_urls\", \"n_sentences\",\n",
      "#             \"n_non_words\", \"idiot_regexp\", \"moron_regexp\", \"n_html\"])\n",
      "#         feature_names.extend([\"strong_pos\", \"strong_neg\", \"weak_pos\",\n",
      "#             \"weak_neg\"])\n",
      "#         feature_names.extend(['n_words', 'n_chars', 'toolong', 'allcaps',\n",
      "#             'max_len', 'mean_len', 'bad_ratio',\n",
      "#             'n_bad', 'capsratio'])\n",
      "#         feature_names = [\" \".join(w) if isinstance(w, tuple) else w\n",
      "#                             for w in feature_names]\n",
      "#         return np.array(feature_names)\n",
      "\n",
      "#     def fit(self, comments, y=None):\n",
      "#         self.fit_transform(comments, y)\n",
      "#         return self\n",
      "\n",
      "#     def fit_transform(self, comments, y=None):\n",
      "#         designed, filtered_words_lower, filtered_words, comments_prep = \\\n",
      "#                 self._preprocess(comments)\n",
      "\n",
      "#         empty_analyzer = lambda x: x\n",
      "#         self.unigram_vect = TfidfVectorizer(analyzer=empty_analyzer, min_df=3)\n",
      "#         print(\"vecorizing\")\n",
      "#         unigrams = self.unigram_vect.fit_transform(filtered_words_lower)\n",
      "\n",
      "#         # pos tag vectorizer\n",
      "#         #self.pos_vect = TfidfVectorizer(analyzer=empty_analyzer).fit(tags)\n",
      "\n",
      "#         # fancy vectorizer\n",
      "#         self.you_are_vect = TfidfVectorizer(\n",
      "#                 token_pattern=\"(?i)you are(?: an?)?(?: the)?(?: as)? (\\w+)\")\n",
      "#         you_are = self.you_are_vect.fit_transform(comments_prep)\n",
      "\n",
      "#         # get the google bad word list\n",
      "#         #with open(\"google_badlist.txt\") as f:\n",
      "#         self.bigram_measures = col.BigramAssocMeasures()\n",
      "#         self.trigram_measures = col.TrigramAssocMeasures()\n",
      "\n",
      "#         # extract bigram collocations including \"you\" (and your?)\n",
      "#         #col.BigramCollocationFinder.from_words([w for c in\n",
      "#         #filtered_words_lower\n",
      "#         #for w in c], window_size=4)\n",
      "\n",
      "#         col_you_bi = col.BigramCollocationFinder.from_documents(\n",
      "#                 filtered_words_lower)\n",
      "#         col_you_bi.apply_freq_filter(3)\n",
      "#         col_you_bi._apply_filter(lambda x, y: np.all([w != \"you\" for w in x]))\n",
      "#         # < 400 of these\n",
      "#         self.you_bigrams = col_you_bi.nbest(self.bigram_measures.chi_sq, 1000)\n",
      "#         self.col_you_bi = col_you_bi\n",
      "#         # make tfidfvectorizer that uses these bigrams\n",
      "#         self.bigram_vect_you = TfidfVectorizer(\n",
      "#                 analyzer=make_collocation_analyzer(self.you_bigrams), min_df=3)\n",
      "#         you_bigrams = self.bigram_vect_you.fit_transform(filtered_words_lower)\n",
      "\n",
      "#         # extract trigram collocations\n",
      "#         col_you_tri = col.TrigramCollocationFinder.from_documents(\n",
      "#                 filtered_words_lower)\n",
      "#         col_you_tri.apply_freq_filter(3)\n",
      "#         col_you_tri._apply_filter(lambda x, y: np.all([w != \"you\" for w in x]))\n",
      "#         # < 400 of these, too\n",
      "#         self.you_trigrams = col_you_tri.nbest(self.trigram_measures.chi_sq,\n",
      "#                                               1000)\n",
      "#         self.col_you_tri = col_you_tri\n",
      "#         self.trigram_vect_you = TfidfVectorizer(\n",
      "#             analyzer=make_collocation_analyzer(self.you_trigrams, 3), min_df=3)\n",
      "#         you_trigrams = self.trigram_vect_you.fit_transform(\n",
      "#                             filtered_words_lower)\n",
      "\n",
      "#         ## some handcrafted features!\n",
      "#         designed.extend(self._handcrafted(filtered_words, comments,\n",
      "#             filtered_words_lower,))\n",
      "#         designed = np.array(designed).T\n",
      "#         self.scaler = MinMaxScaler()\n",
      "#         designed = self.scaler.fit_transform(designed)\n",
      "#         features = []\n",
      "#         features.append(unigrams)\n",
      "#         features.append(you_bigrams)\n",
      "#         features.append(you_trigrams)\n",
      "#         features.append(you_are)\n",
      "#         #features.append(pos_unigrams)\n",
      "#         features.append(sparse.csr_matrix(designed))\n",
      "#         features = sparse.hstack(features).tocsr()\n",
      "\n",
      "#         return features\n",
      "\n",
      "#     def _preprocess(self, comments):\n",
      "#         # remove nicknames, urls, html\n",
      "#         nick = re.compile(ur\"@\\w\\w+:?\")\n",
      "#         url = re.compile(ur\"http[^\\s]*\")\n",
      "#         html = re.compile(ur\"</?\\w+[^>]*>\")\n",
      "#         n_html = [len(html.findall(c)) for c in comments]\n",
      "#         comments = [html.sub(' ', c) for c in comments]\n",
      "\n",
      "#         n_nicks = [len(nick.findall(c)) for c in comments]\n",
      "#         comments_nonick = [nick.sub('', c) for c in comments]\n",
      "\n",
      "#         n_urls = [len(url.findall(c)) for c in comments_nonick]\n",
      "#         comments_nourl = [url.sub(' ', c) for c in comments_nonick]\n",
      "#         comments_ascii = [c.replace(u'\\xa0', ' ') for c in comments_nourl]\n",
      "#         comments_ascii = [remove_non_ascii(c) for c in comments_ascii]\n",
      "#         comments_ascii = [\n",
      "#             c.replace(\"'ll\", \"will\").replace(\"n't\", \"not\")\n",
      "#             .replace(\"'LL\", \"WILL\").replace(\"N'T\", \"NOT\")\n",
      "#             for c in comments_ascii]\n",
      "#         # replace /  with space, as this often separates words\n",
      "#         comments_ascii = [c.replace(u'/', ' ') for c in comments_ascii]\n",
      "\n",
      "#         ur = \"you are \"\n",
      "#         UR = \"YOU ARE \"\n",
      "#         comments_ascii = [re.sub(ur\"[Yy]ou'? ?a?re \", ur, c)\n",
      "#                 for c in comments_ascii]\n",
      "#         # again for the loud people (don't want to lose that)\n",
      "#         comments_ascii = [re.sub(ur\"YOU'? ?A?RE \", UR, c)\n",
      "#                 for c in comments_ascii]\n",
      "#         idiot = [len(re.findall(\"you.? [\\w ]* idi.t\", c))\n",
      "#                 for c in comments_ascii]\n",
      "#         moron = [len(re.findall(\"you.? [\\w ]* m.r.n\", c))\n",
      "#                 for c in comments_ascii]\n",
      "#         # split into sentences\n",
      "#         sentences = [nltk.sent_tokenize(comment)\n",
      "#                      for comment in comments_ascii]\n",
      "#         # remove dots as they are annoying\n",
      "#         sentences = [[s.replace(\".\", \" \") for s in sent] for sent in\n",
      "#                 sentences]\n",
      "#         #punctuation = \\\n",
      "#             #['...', '.', '?', '!', ',', \"''\", '``', '#', '$', \"'\", \"%\", \"&\"]\n",
      "#         n_sentences = [len(sent) for sent in sentences]\n",
      "#         words = [[nltk.word_tokenize(s) for s in sent] for sent in sentences]\n",
      "#         #tagged = [[nltk.pos_tag(s) for s in comment] for comment in words]\n",
      "#         #tags = [[tag[1] for sent in comment for tag in sent]\n",
      "#                 #for comment in tagged]\n",
      "#         flat_words = [[w for sent in sents for w in sent] for sents in words]\n",
      "#         # remove \"words\" that contain no letter/numbers\n",
      "#         filtered_words = [[w for w in c\n",
      "#             if not re.findall(r\"^[^\\w]*$\", w)] for c in flat_words]\n",
      "\n",
      "#         # get rid of non-word characters sourrounding words\n",
      "#         filtered_words = [[re.sub(\"^[^\\w]*(\\w+)[^\\w]*$\", r\"\\1\", w) for w in c]\n",
      "#                 for c in filtered_words]\n",
      "#         # laughter normalization ^^\n",
      "#         filtered_words = [[re.sub(\"(?i)ha(ha)+\", r\"haha\", w) for w in c]\n",
      "#                 for c in filtered_words]\n",
      "#         filtered_words = [[re.sub(\"(?i)l+o+l+(o+l+)+\", r\"lol\", w) for w in c]\n",
      "#                 for c in filtered_words]\n",
      "#         # replace the famous \"0\" as o\n",
      "#         filtered_words = [[re.sub(\"(?i)([a-z]+)0([a-z]+)\", r\"\\1O\\2\", w)\n",
      "#             for w in c] for c in filtered_words]\n",
      "\n",
      "#         filtered_words = [[self.stemmer.stem(w)\n",
      "#             for w in c] for c in filtered_words]\n",
      "\n",
      "#         # detect weird stuff so we can spellcheck\n",
      "#         non_words = [[a for a in s if not self.d.check(a)]\n",
      "#                       for s in filtered_words]\n",
      "#         non_words = [[a for a in s if not nltk.corpus.wordnet.synsets(a)]\n",
      "#                     for s in non_words]\n",
      "#         non_words = [[a for a in s if not a.lower() in self.badwords_]\n",
      "#                     for s in non_words]\n",
      "#         n_non_words = [len(w) for w in non_words]\n",
      "#         filtered_words_lower = [[w.lower() for w in comment]\n",
      "#                 for comment in filtered_words]\n",
      "#         #flat = [a for s in non_words for a in s]\n",
      "#         #bla, blub = np.unique(flat, return_inverse=True)\n",
      "#         #not words, only there once. we could try and guess?\n",
      "#         #to_replace = bla[np.bincount(blub) == 1].tolist()\n",
      "#         #tracer()\n",
      "#         features = [n_nicks, n_urls, n_sentences, n_non_words, idiot, moron,\n",
      "#                 n_html]\n",
      "#         return [features, filtered_words_lower,\n",
      "#                 filtered_words, comments_ascii]\n",
      "\n",
      "#     def _handcrafted(self, filtered_words, comments, filtered_words_lower):\n",
      "#         ## some handcrafted features!\n",
      "#         n_words = np.array([len(c) for c in filtered_words], dtype=np.float)\n",
      "#         n_words += 0.1\n",
      "#         n_chars = [len(c) for c in comments]\n",
      "#         too_long = np.array(n_chars) > 1000\n",
      "#         # number of uppercase words\n",
      "#         allcaps = [np.sum([w.isupper() for sentence in comment\n",
      "#                            for w in sentence])\n",
      "#                    for comment in filtered_words]\n",
      "#         # longest word\n",
      "#         # after removeing all the stuff above, the comment migh be empty\n",
      "#         max_word_len = [np.max([len(w) for w in c])\n",
      "#                 if len(c) else 0 for c in filtered_words]\n",
      "#         # average word length\n",
      "#         mean_word_len = [np.mean([len(w) for w in c])\n",
      "#                 if len(c) else 0 for c in filtered_words]\n",
      "\n",
      "#         # number of google badwords:\n",
      "#         # also take plurals\n",
      "#         #n_bad = [np.sum([c.lower().count(w) + c.lower().count(w + \"s\")\n",
      "#                  #for w in self.badwords_])\n",
      "#                  #if len(c) else 0 for c in comments]\n",
      "#         #n_bad = [np.sum([c.lower().count(w)\n",
      "#                  #for w in self.badwords_])\n",
      "#                  #if len(c) else 0 for c in comments]\n",
      "\n",
      "#         n_bad = [np.sum([self.stemmer.stem_word(w) in self.badwords_\n",
      "#             for w in c])\n",
      "#                  if len(c) else 0 for c in filtered_words_lower]\n",
      "\n",
      "#         allcaps_ratio = np.array(allcaps) / n_words\n",
      "#         bad_ratio = np.array(n_bad) / n_words\n",
      "\n",
      "#         # subjectivity database\n",
      "#         strong_pos = [np.sum([w in self.subjectivity[0] for w in c])\n",
      "#                 if len(c) else 0 for c in filtered_words_lower]\n",
      "#         strong_pos = np.array(strong_pos) / n_words\n",
      "#         strong_neg = [np.sum([w in self.subjectivity[1] for w in c])\n",
      "#                 if len(c) else 0 for c in filtered_words_lower]\n",
      "#         strong_neg = np.array(strong_pos) / n_words\n",
      "#         weak_pos = [np.sum([w in self.subjectivity[2] for w in c])\n",
      "#                 if len(c) else 0 for c in filtered_words_lower]\n",
      "#         weak_pos = np.array(strong_pos) / n_words\n",
      "#         weak_neg = [np.sum([w in self.subjectivity[3] for w in c])\n",
      "#                 if len(c) else 0 for c in filtered_words_lower]\n",
      "#         weak_neg = np.array(strong_pos) / n_words\n",
      "\n",
      "#         result = [strong_pos, strong_neg, weak_pos, weak_neg, n_words, n_chars,\n",
      "#                 allcaps, too_long, max_word_len, mean_word_len, bad_ratio,\n",
      "#                 n_bad, allcaps_ratio]\n",
      "#         return result\n",
      "\n",
      "#     def transform(self, comments):\n",
      "#         designed, filtered_words_lower, filtered_words, comments_prep = \\\n",
      "#                 self._preprocess(comments)\n",
      "\n",
      "#         # get started with real features:\n",
      "#         unigrams = self.unigram_vect.transform(filtered_words_lower)\n",
      "#         you_bigrams = self.bigram_vect_you.transform(filtered_words_lower)\n",
      "#         you_trigrams = self.trigram_vect_you.transform(filtered_words_lower)\n",
      "#         #pos_unigrams = self.pos_vect.transform(tags)\n",
      "#         you_are = self.you_are_vect.transform(comments_prep)\n",
      "\n",
      "#         ## some handcrafted features!\n",
      "#         designed.extend(self._handcrafted(filtered_words, comments,\n",
      "#             filtered_words_lower))\n",
      "#         designed = np.array(designed).T\n",
      "#         designed = self.scaler.transform(designed)\n",
      "\n",
      "#         features = []\n",
      "#         features.append(unigrams)\n",
      "#         features.append(you_bigrams)\n",
      "#         features.append(you_trigrams)\n",
      "#         features.append(you_are)\n",
      "#         #features.append(pos_unigrams)\n",
      "#         features.append(sparse.csr_matrix(designed))\n",
      "#         features = sparse.hstack(features).tocsr()\n",
      "\n",
      "#         return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}